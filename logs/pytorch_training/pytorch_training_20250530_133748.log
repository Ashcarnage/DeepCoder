2025-05-30 13:37:48,018 - AgenticTrainer - INFO - Training log: logs/pytorch_training/pytorch_training_20250530_133748.log
2025-05-30 13:37:48,045 - AgenticTrainer - INFO - Loading model: Qwen/Qwen2.5-1.5B-Instruct
2025-05-30 13:38:22,522 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-05-30 13:38:24,165 - AgenticTrainer - INFO - Model loaded with LoRA configuration
2025-05-30 13:38:24,166 - AgenticTrainer - INFO - Model dtype: torch.float16
2025-05-30 13:38:24,167 - AgenticTrainer - INFO - Device: cuda:0
2025-05-30 13:38:24,175 - AgenticTrainer - INFO - Training samples: 45
2025-05-30 13:38:24,176 - AgenticTrainer - INFO - Validation samples: 5
2025-05-30 13:38:24,178 - AgenticTrainer - INFO - Optimizer setup with lr=0.0002
2025-05-30 13:38:24,179 - AgenticTrainer - INFO - ðŸš€ Starting PyTorch Agentic Training with LoRA and SAD!
2025-05-30 13:38:24,179 - AgenticTrainer - INFO - ============================================================
2025-05-30 13:38:27,372 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-30 13:38:27,780 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-05-30 13:38:27,781 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-05-30 13:38:35,870 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-30 13:38:36,190 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-05-30 13:38:36,191 - AgenticTrainer - WARNING - Teacher model error: Error code: 413 - {'error': {'message': 'Request too large for model `deepseek-r1-distill-llama-70b` in organization `org_01j5jb0kb9e7qv5erhdyh9wcdm` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 10625, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-05-30 13:38:36,447 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-05-30 13:38:36,469 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds
2025-05-30 13:38:40,508 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-30 13:38:40,812 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-05-30 13:38:40,813 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds
2025-05-30 13:38:44,175 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-30 13:38:44,486 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-05-30 13:38:44,487 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 60.000000 seconds
2025-05-30 13:39:47,045 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-30 13:39:47,527 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
